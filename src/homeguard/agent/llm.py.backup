"""LLM provider abstraction using litellm or backend proxy."""

import json
import httpx
from typing import Optional
from .config import LLMConfig, DEFAULT_BACKEND_URL

SYSTEM_PROMPT = """You are HomeGuard, an expert network security analyst. Your job is to thoroughly scan and analyze a home network.

## REQUIRED WORKFLOW (Follow exactly)
1. Call scan_network to discover all devices
2. For EACH device found, call scan_ports with the device's IP
3. IDENTIFY device types and run appropriate DEEP SCANS
4. For "Unknown Device" types, use probe_unknown_device to identify them
5. After all scans complete, call generate_report with your findings

## Your Tools
- scan_network: Discover all devices (returns IP, MAC, OS guess)
- scan_ports(ip, mode): Scan device ports. Mode is set by user config.
- get_service_info(port): Get details about a port's service and risks
- lookup_cve(keyword): Search for vulnerabilities (e.g., "apache", "ssh")
- generate_report(findings, recommendations, risk_level): MUST call this at the end
- suggest_fix(issue, command, explanation): Suggest fixes (requires user approval)

## DEEP SCAN TOOLS (Use based on device type!)
- deep_scan_router(ip): For routers/gateways - checks UPnP, Telnet, TR-069, DNS
- deep_scan_iot(ip): For IoT devices - checks MQTT, RTSP, default ports
- deep_scan_storage(ip): For NAS/storage - checks SMB, NFS, FTP, shares
- probe_unknown_device(ip): For unknown devices - extended port scan, banner grab, HTTP fingerprint
- check_default_credentials(ip, port): Check web interface for default creds

## Device Identification & Deep Scan Strategy
After port scanning, identify device type and run the appropriate deep scan:

| Device Type | Indicators | Deep Scan |
|-------------|------------|-----------|
| Router | Gateway IP (.1/.254), ports 53/80/443 | deep_scan_router |
| IoT | Ports 1883/8883 (MQTT), 554 (RTSP) | deep_scan_iot |
| NAS/Storage | Ports 445/139/2049/5000 | deep_scan_storage |
| Smart TV | Ports 8008/9000 | deep_scan_iot |
| Camera | Port 554 (RTSP) | deep_scan_iot |
| Printer | Ports 9100/631 | (basic scan sufficient) |
| **Unknown** | No clear indicators | **probe_unknown_device** |

## IMPORTANT: Unknown Device Handling
When a device is identified as "Unknown Device", ALWAYS call probe_unknown_device(ip) to:
- Scan extended port range (databases, gaming, Apple, etc.)
- Grab service banners for identification
- Check HTTP headers and page titles
- Identify device type with higher confidence

## Risk Assessment
- CRITICAL: Telnet (23), TR-069 (7547), unencrypted MQTT (1883), exposed databases
- HIGH: SMBv1, FTP (21), SNMP (161), VNC without auth
- MEDIUM: UPnP enabled, HTTP admin without HTTPS
- LOW: HTTPS only, no unnecessary open ports

## Report Requirements
Your generate_report MUST include:
- findings: List each device with type, deep scan results, and security issues
- recommendations: Specific actions based on deep scan findings
- risk_level: Overall network risk (low/medium/high/critical)

START NOW: Call scan_network, then scan_ports for each device, then appropriate deep scans (including probe_unknown_device for unknowns), then generate_report."""


async def chat_with_backend(messages: list[dict], tools: list[dict], base_url: str) -> dict:
    """Send chat request to HomeGuard backend."""
    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(
            f"{base_url}/api/chat",
            json={"messages": messages, "tools": tools},
        )
        response.raise_for_status()
        data = response.json()

        # Convert backend response to litellm-like format
        class FakeChoice:
            def __init__(self, msg_data, finish):
                self.message = FakeMessage(msg_data)
                self.finish_reason = finish

        class FakeMessage:
            def __init__(self, data):
                self.content = data.get("content")
                self.role = data.get("role")
                self.tool_calls = None
                if data.get("tool_calls"):
                    self.tool_calls = [FakeToolCall(tc) for tc in data["tool_calls"]]

        class FakeToolCall:
            def __init__(self, data):
                self.id = data["id"]
                self.type = data["type"]
                self.function = FakeFunction(data["function"])

        class FakeFunction:
            def __init__(self, data):
                self.name = data["name"]
                self.arguments = data["arguments"]

        class FakeResponse:
            def __init__(self, data):
                self.choices = [FakeChoice(data["message"], data["finish_reason"])]

        return FakeResponse(data)


def chat_with_tools(
    messages: list[dict],
    config: LLMConfig,
) -> dict:
    """Send messages to LLM and get response with potential tool calls."""
    from .tools import TOOL_DEFINITIONS

    # Use backend proxy
    if config.is_backend_proxy:
        import asyncio
        base_url = config.base_url or DEFAULT_BACKEND_URL
        try:
            return asyncio.get_event_loop().run_until_complete(
                chat_with_backend(messages, TOOL_DEFINITIONS, base_url)
            )
        except RuntimeError:
            # No event loop running
            return asyncio.run(chat_with_backend(messages, TOOL_DEFINITIONS, base_url))

    # Direct LLM call
    from litellm import completion

    if config.provider == "ollama":
        model = f"ollama/{config.model}"
    elif config.provider == "deepseek":
        model = f"deepseek/{config.model}"
    elif config.provider == "bedrock":
        model = f"bedrock/{config.model}"
    elif config.provider == "anthropic":
        model = f"anthropic/{config.model}"
    else:
        model = config.model

    try:
        response = completion(
            model=model,
            messages=messages,
            tools=TOOL_DEFINITIONS,
            api_key=config.api_key,
            base_url=config.base_url,
        )
        return response
    except Exception as e:
        raise RuntimeError(f"LLM error: {e}")


def extract_tool_calls(response: dict) -> list[dict]:
    """Extract tool calls from LLM response."""
    message = response.choices[0].message
    if hasattr(message, "tool_calls") and message.tool_calls:
        return [
            {
                "id": tc.id,
                "name": tc.function.name,
                "arguments": json.loads(tc.function.arguments),
            }
            for tc in message.tool_calls
        ]
    return []


def get_response_text(response: dict) -> Optional[str]:
    """Get text content from LLM response."""
    message = response.choices[0].message
    return message.content
